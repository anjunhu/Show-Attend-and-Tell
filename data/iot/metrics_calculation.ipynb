{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "cider_scorer = Cider()\n",
    "bleu_scorer = Bleu(4)\n",
    "meteor_scorer = Meteor()\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../model_cocopretrain_iotfinetune/model_resnet152_5.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../../model_cocopretrain_iotfinetune/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths and their corresponding column names in a dictionary (flipped)\n",
    "file_paths = {\n",
    "    'vgg19': 'model_vgg19_10_prediction_IMERIT_2024_03_25_TEST_v5.csv',\n",
    "    'rn152': 'model_resnet152_10_prediction_IMERIT_2024_03_25_TEST_v5.csv',\n",
    "    'target': 'metadata_IMERIT_2024_03_25_TEST_v5.csv',\n",
    "    'rn152_ft_highlr_failed': '../../model_cocopretrain_iotfinetune/model_resnet152_5.csv',\n",
    "    'rn152_ft_1': '../../model_cocopretrain_iotfinetune_2/model_resnet152_1.csv',\n",
    "    'rn152_ft_5': '../../model_cocopretrain_iotfinetune_2/model_resnet152_5.csv',\n",
    "    'rn152_ft_10': '../../model_cocopretrain_iotfinetune_2/model_resnet152_10.csv',\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the file paths and load the CSV files\n",
    "for column_name, path in file_paths.items():\n",
    "    df = pd.read_csv(path).rename(columns={'text': column_name})\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Start merging with the first dataframe\n",
    "merged_df = dataframes[0]\n",
    "\n",
    "# Loop through the rest of the dataframes and merge them on 'file_name'\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on='file_name')\n",
    "\n",
    "# Now `merged_df` contains the merged data from all the CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df \n",
    "merged_df.to_csv(\"caption_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- VGG19 --------------------\n",
      "CIDEr 0.036490947241140666\n",
      "{'testlen': 7167, 'reflen': 11664, 'guess': [7167, 6185, 5203, 4221], 'correct': [959, 51, 0, 0]}\n",
      "ratio: 0.6144547325102354\n",
      "BLEU [0.07144620078661279, 0.01773592217667285, 3.184079383057296e-08, 4.4952987680319783e-11]\n",
      "ROUGE 0.09457508711908663 \n",
      "\n",
      "-------------------- RN152 --------------------\n",
      "CIDEr 0.038019906963850575\n",
      "{'testlen': 7167, 'reflen': 11985, 'guess': [7167, 6185, 5203, 4221], 'correct': [984, 44, 3, 1]}\n",
      "ratio: 0.597997496871039\n",
      "BLEU [0.07009776689995645, 0.01595628637425042, 0.0042162451443005285, 0.0017352105308910395]\n",
      "ROUGE 0.09702593981792966 \n",
      "\n",
      "-------------------- TARGET --------------------\n",
      "CIDEr 10.0\n",
      "{'testlen': 7167, 'reflen': 7167, 'guess': [7167, 6185, 5203, 4221], 'correct': [7167, 6185, 5203, 4221]}\n",
      "ratio: 0.9999999999998604\n",
      "BLEU [0.9999999999997208, 0.9999999999997097, 0.9999999999996958, 0.9999999999996777]\n",
      "ROUGE 1.0 \n",
      "\n",
      "-------------------- RN152_FT_HIGHLR_FAILED --------------------\n",
      "CIDEr 0.03701131470328819\n",
      "{'testlen': 7167, 'reflen': 6496, 'guess': [7167, 6185, 5203, 4221], 'correct': [1488, 59, 0, 0]}\n",
      "ratio: 1.1032943349751996\n",
      "BLEU [0.2076182503139099, 0.044502961976248846, 7.247273805896959e-08, 9.74489500180425e-11]\n",
      "ROUGE 0.18060261574931885 \n",
      "\n",
      "-------------------- RN152_FT_1 --------------------\n",
      "CIDEr 0.03900266461886101\n",
      "{'testlen': 7167, 'reflen': 11979, 'guess': [7167, 6185, 5203, 4221], 'correct': [1005, 48, 5, 1]}\n",
      "ratio: 0.5982970197845732\n",
      "BLEU [0.07165371709373336, 0.016856801112347244, 0.005186712676463942, 0.0020272990723313373]\n",
      "ROUGE 0.09244760488235505 \n",
      "\n",
      "-------------------- RN152_FT_5 --------------------\n",
      "CIDEr 0.033313673048158965\n",
      "{'testlen': 7167, 'reflen': 12210, 'guess': [7167, 6185, 5203, 4221], 'correct': [1044, 40, 3, 1]}\n",
      "ratio: 0.5869778869778389\n",
      "BLEU [0.07207346314832531, 0.015186370360299576, 0.004037035481610659, 0.0016664648621593692]\n",
      "ROUGE 0.09430727182209986 \n",
      "\n",
      "-------------------- RN152_FT_10 --------------------\n",
      "CIDEr 0.032850515118839935\n",
      "{'testlen': 7167, 'reflen': 12162, 'guess': [7167, 6185, 5203, 4221], 'correct': [1055, 41, 4, 0]}\n",
      "ratio: 0.5892945239269373\n",
      "BLEU [0.07332228305580105, 0.015559675234412675, 0.004525946144575419, 3.234137122452994e-07]\n",
      "ROUGE 0.09543967797024618 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in file_paths.keys():\n",
    "    print('-'*20, model_name.upper(), '-'*20)\n",
    "    refer_captions = {}\n",
    "    predc_captions = {}\n",
    "    for i in range(len(merged_df)):\n",
    "        refer_captions[merged_df.iloc[i]['file_name']] = [str(merged_df.iloc[i][model_name])]\n",
    "        predc_captions[merged_df.iloc[i]['file_name']] = [str(merged_df.iloc[i]['target'])]\n",
    "    c, _ = cider_scorer.compute_score(refer_captions, predc_captions); print(\"CIDEr\", c)\n",
    "    b, _ = bleu_scorer.compute_score(refer_captions, predc_captions); print('BLEU', b)\n",
    "    r, _ = rouge_scorer.compute_score(refer_captions, predc_captions); print('ROUGE', r, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
